{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "### Our processing scripts\n",
        "\n",
        "#### Download the original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pwd\n",
        "%cd ../../\n",
        "# set temporary path\n",
        "%env PYTHONPATH=$PYTHONPATH:$(pwd)\n",
        "%cd ./examples/ppdiffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "# from ppcfd.data import download\n",
        "import importlib\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "download_module = importlib.import_module('ppcfd.data.downloader')\n",
        "\n",
        "SINGLE = \"https://archive.nyu.edu/bitstream/2451/63316/4/nn-benchmark-data-navier-stokes-single.tar.gz\"\n",
        "MULTI = \"https://archive.nyu.edu/bitstream/2451/63316/5/nn-benchmark-data-navier-stokes-multi.tar.gz\"\n",
        "urls = [SINGLE, MULTI]\n",
        "cache_dir = Path(\"./data/tars/\")\n",
        "\n",
        "print(\"Start downloading...\")\n",
        "download_module.download(\n",
        "    urls,\n",
        "    cache_dir=cache_dir,\n",
        "    force_redownload=True,\n",
        "    use_parallel=True,\n",
        "    max_workers=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": []
      },
      "source": [
        "#### Transform data and save to .h5 files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "from collections import namedtuple\n",
        "from pathlib import Path\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "Trajectory = namedtuple(\n",
        "    \"Trajectory\",\n",
        "    [\n",
        "        \"name\",\n",
        "        \"features\",\n",
        "        \"dp_dt\",\n",
        "        \"dq_dt\",\n",
        "        \"t\",\n",
        "        \"trajectory_meta\",\n",
        "        \"p_noiseless\",\n",
        "        \"q_noiseless\",\n",
        "        \"masses\",\n",
        "        \"edge_index\",\n",
        "        \"vertices\",\n",
        "        \"fixed_mask\",\n",
        "        \"condition\",\n",
        "        \"static_nodes\",\n",
        "    ],\n",
        ")\n",
        "\n",
        "\n",
        "class TrajectoryDataset:\n",
        "    \"\"\"Returns batches of full trajectories.\n",
        "\n",
        "    dataset[idx] -> a set of snapshots for a full trajectory\n",
        "\n",
        "    For 'navier-stokes':\n",
        "    core values of this system: solutions(x) and pressures(y)\n",
        "    metadata(dict).keys()=['system', 'system_args', 'metadata', 'trajectories']\n",
        "    'system' is 'navier-stokes'\n",
        "    'system_args.trajectory_defs' has 'viscosity' and 'in_velocity',\n",
        "        which are parameters passed to the FEM solver giving the viscosity of\n",
        "        the fluid and the velocity of the incoming flow\n",
        "    'metadata' has 'grid_resolution' and 'viscosity'\n",
        "    'trajectories' have 'in_velocity', and 'viscosity'\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_dir, subsample: int = 1, max_samples: int = None):\n",
        "        super().__init__()\n",
        "        data_dir = Path(data_dir)\n",
        "        self.subsample = subsample\n",
        "        self.max_samples = max_samples\n",
        "\n",
        "        with open(data_dir / \"system_meta.json\", \"r\", encoding=\"utf8\") as meta_file:\n",
        "            metadata = json.load(meta_file)\n",
        "\n",
        "        self.system = metadata[\"system\"]\n",
        "        self.system_metadata = metadata[\"metadata\"]\n",
        "        self._trajectory_meta = metadata[\"trajectories\"]\n",
        "        self._npz_file = np.load(data_dir / \"trajectories.npz\")\n",
        "        if self.system == \"navier-stokes\":\n",
        "            self.h, self.w = 221, 42\n",
        "            self._ndims_p = 2\n",
        "            self._ndims_q = 1\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown system: {self.system}\")\n",
        "\n",
        "    def concatenate_features(self, p, q, channel_dim=-1):\n",
        "        \"\"\"How to concatenate any item in the dataset\"\"\"\n",
        "        q = np.expand_dims(q, axis=channel_dim) if q.shape[channel_dim] != 1 and q.ndim <= 2 else q\n",
        "        assert p.shape[channel_dim] == 2, f\"Expected p to have 2 channels but got {p.shape}\"\n",
        "        assert q.shape[channel_dim] == self._ndims_q, f\"Expected q to have {self._ndims_q} channel, but got {q.shape}\"\n",
        "        dynamics = np.concatenate([p, q], axis=channel_dim)\n",
        "        return dynamics\n",
        "\n",
        "    def get_others(self, field_keys, p, q):\n",
        "        dp_dt = self._npz_file[field_keys[\"dpdt\"]]\n",
        "        dq_dt = self._npz_file[field_keys[\"dqdt\"]]\n",
        "\n",
        "        # Handle (possibly missing) noiseless data\n",
        "        p_key = \"p_noiseless\" if \"p_noiseless\" in field_keys else \"p\"\n",
        "        q_key = \"q_noiseless\" if \"q_noiseless\" in field_keys else \"q\"\n",
        "        p_noiseless = self._npz_file[field_keys[p_key]]\n",
        "        q_noiseless = self._npz_file[field_keys[q_key]]\n",
        "\n",
        "        # Handle (possibly missing) masses\n",
        "        masses = self._npz_file[field_keys[\"masses\"]] if \"masses\" in field_keys else np.ones(p.shape[1])\n",
        "        if \"edge_indices\" in field_keys:\n",
        "            edge_index = self._npz_file[field_keys[\"edge_indices\"]]\n",
        "            if edge_index.shape[0] != 2:\n",
        "                edge_index = edge_index.T\n",
        "        else:\n",
        "            edge_index = []\n",
        "        vertices = self._npz_file[field_keys[\"vertices\"]] if \"vertices\" in field_keys else []\n",
        "\n",
        "        # Handle per-trajectory boundary masks\n",
        "        fixed_mask_p = (\n",
        "            np.expand_dims(self._npz_file[field_keys[\"fixed_mask_p\"]], 0) if \"fixed_mask_p\" in field_keys else []\n",
        "        )\n",
        "        fixed_mask_q = (\n",
        "            np.expand_dims(self._npz_file[field_keys[\"fixed_mask_q\"]], 0) if \"fixed_mask_q\" in field_keys else []\n",
        "        )\n",
        "        extra_fixed_mask = (\n",
        "            np.expand_dims(self._npz_file[field_keys[\"extra_fixed_mask\"]], 0)\n",
        "            if \"extra_fixed_mask\" in field_keys\n",
        "            else []\n",
        "        )\n",
        "        static_nodes = (\n",
        "            np.expand_dims(self._npz_file[field_keys[\"enumerated_fixed_mask\"]], 0)\n",
        "            if \"enumerated_fixed_mask\" in field_keys\n",
        "            else []\n",
        "        )\n",
        "\n",
        "        # set parrerns\n",
        "        efm_pattern = \"1 (h w) c -> c h w\" if extra_fixed_mask.ndim == 3 else \"1 (h w) -> 1 h w\"\n",
        "        q_pattern = \"time (h w) -> time 1 h w\" if q.ndim == 2 else \"time (h w) c -> time c h w\"\n",
        "        q_static_pattern = \"(h w) -> h w\" if q.ndim == 2 else \"(h w) c -> c h w\"\n",
        "\n",
        "        extra_fixed_mask = rearrange(extra_fixed_mask, efm_pattern, h=self.h, w=self.w)\n",
        "        dp_dt = rearrange(dp_dt, \"time (h w) c -> time c h w\", h=self.h, w=self.w).astype(np.float32)\n",
        "        dq_dt = rearrange(dq_dt, q_pattern, h=self.h, w=self.w).astype(np.float32)\n",
        "        p_noiseless = rearrange(p_noiseless, \"time (h w) c -> time c h w\", h=self.h, w=self.w).astype(np.float32)\n",
        "        q_noiseless = rearrange(q_noiseless, q_pattern, h=self.h, w=self.w).astype(np.float32)\n",
        "\n",
        "        masses = rearrange(masses, \"(h w) -> h w\", h=self.h, w=self.w)\n",
        "        vertices = (\n",
        "            rearrange(vertices, \"(h w) c -> c h w\", h=self.h, w=self.w).astype(np.float32) if len(vertices) > 0 else []\n",
        "        )\n",
        "        static_nodes = (\n",
        "            rearrange(static_nodes.squeeze(), \"(h w) -> h w\", h=self.h, w=self.w) if len(static_nodes[0]) > 0 else []\n",
        "        )\n",
        "        fixed_mask_p = rearrange(fixed_mask_p.squeeze(), \"(h w) c -> c h w\", h=self.h, w=self.w)\n",
        "        fixed_mask_q = rearrange(fixed_mask_q.squeeze(), q_static_pattern, h=self.h, w=self.w)\n",
        "        fixed_mask = self.concatenate_features(fixed_mask_p, q=fixed_mask_q, channel_dim=0)\n",
        "        return (\n",
        "            extra_fixed_mask,\n",
        "            dp_dt,\n",
        "            dq_dt,\n",
        "            p_noiseless,\n",
        "            q_noiseless,\n",
        "            masses,\n",
        "            vertices,\n",
        "            static_nodes,\n",
        "            fixed_mask,\n",
        "            edge_index,\n",
        "        )\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        meta = self._trajectory_meta[idx]\n",
        "        name = meta[\"name\"]\n",
        "        field_keys = meta[\"field_keys\"]\n",
        "\n",
        "        # Load arrays\n",
        "        # print([self._trajectory_meta[idx][\"name\"] for idx in range(self.__len__())])\n",
        "        # print(f'----> Loading trajectory {name}', field_keys[\"p\"])\n",
        "\n",
        "        p = self._npz_file[field_keys[\"p\"]]\n",
        "        q = self._npz_file[field_keys[\"q\"]]\n",
        "        # concatenate the p, q variables into the feature dimension\n",
        "        features = self.concatenate_features(p, q, channel_dim=-1)\n",
        "        # reconstruct spatial dimensions, from (time, 221*42, channel) to (time, channel, 221, 42)\n",
        "        features = rearrange(features, \"time (h w) c -> time c h w\", h=self.h, w=self.w).astype(np.float32)\n",
        "\n",
        "        t = self._npz_file[field_keys[\"t\"]]\n",
        "        (\n",
        "            extra_fixed_mask,\n",
        "            dp_dt,\n",
        "            dq_dt,\n",
        "            p_noiseless,\n",
        "            q_noiseless,\n",
        "            masses,\n",
        "            vertices,\n",
        "            static_nodes,\n",
        "            fixed_mask,\n",
        "            edge_index,\n",
        "        ) = self.get_others(field_keys, p, q)\n",
        "\n",
        "        # Package and return\n",
        "        return Trajectory(\n",
        "            name=name,\n",
        "            trajectory_meta=meta,\n",
        "            features=features,\n",
        "            dp_dt=dp_dt,\n",
        "            dq_dt=dq_dt,\n",
        "            t=t,\n",
        "            p_noiseless=p_noiseless,\n",
        "            q_noiseless=q_noiseless,\n",
        "            masses=masses,\n",
        "            edge_index=edge_index,\n",
        "            vertices=vertices,\n",
        "            fixed_mask=fixed_mask,\n",
        "            condition=extra_fixed_mask,\n",
        "            static_nodes=static_nodes,\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._trajectory_meta) if self.max_samples is None else self.max_samples\n",
        "\n",
        "\n",
        "def save_to_h5(dataset, output_path: str):\n",
        "    with h5py.File(output_path, \"w\") as h5_file:\n",
        "        for idx in range(len(dataset)):\n",
        "            traj = dataset[idx]  # get Trajectory item\n",
        "            group = h5_file.create_group(f\"trajectory_{idx}\")  # split by index\n",
        "\n",
        "            # store base array\n",
        "            for key in [\"features\", \"dp_dt\", \"dq_dt\", \"t\", \"p_noiseless\",\n",
        "                        \"q_noiseless\", \"masses\", \"edge_index\", \"vertices\",\n",
        "                        \"fixed_mask\", \"condition\", \"static_nodes\"]:\n",
        "                group.create_dataset(key, data=getattr(traj, key))\n",
        "\n",
        "            # deal with 'trajectory_meta'\n",
        "            meta_group = group.create_group(\"trajectory_meta\")\n",
        "            for k, v in traj.trajectory_meta.items():\n",
        "                if isinstance(v, dict):\n",
        "                    # serialized nested dictionaries as JSON strings\n",
        "                    data = json.dumps(v).encode(\"utf-8\")\n",
        "                    meta_group.create_dataset(k, data=data)\n",
        "                elif isinstance(v, str):\n",
        "                    # convert string to HDF5 compatible format\n",
        "                    meta_group.create_dataset(k, data=np.bytes_(v))\n",
        "                else:\n",
        "                    # store scalar values directly\n",
        "                    meta_group.create_dataset(k, data=np.array(v))\n",
        "\n",
        "            # store 'name'\n",
        "            group.create_dataset(\"name\", data=np.bytes_(traj.name))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    types = [\"single\", \"multi\"]\n",
        "    for task_type in types:\n",
        "        root_dir = f\"./data/decompressed/navier-stokes-{task_type}/run/data_gen\"\n",
        "        save_dir = f\"./data/h5/{task_type}\"\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        for dirname in os.listdir(root_dir):\n",
        "            file_dir = os.path.join(root_dir, dirname)\n",
        "            file_name = os.path.join(save_dir, f\"{dirname}.h5\")\n",
        "            print(f\"Start trans {file_name}\")\n",
        "            dataset = TrajectoryDataset(file_dir)\n",
        "            save_to_h5(dataset, file_name)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "py35-paddle1.2.0"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "09f0dbf7b1569c1ab842ae2f41770fe6aa1b54326d081112fa5944b99abb5899"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
